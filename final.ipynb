{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a02fb6-c176-49b8-a659-b2804ef51f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PART 1/3: Final Corrected Code ===\n",
    "import os\n",
    "import fitz\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from paddleocr import PPStructureV3\n",
    "from PIL import Image\n",
    "\n",
    "# === CONFIG ===\n",
    "PDF_FOLDER = r\"C:\\Users\\Admin\\Desktop\\deep books\"\n",
    "BASE_OUTPUT = r\"C:\\Users\\Admin\\Desktop\\vbooks\"\n",
    "DPI = 300\n",
    "TRAIN_RATIO = 0.9\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# === LOGGING ===\n",
    "def setup_logger():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    "    )\n",
    "\n",
    "def safe_mkdir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# === 1) PDF â†’ IMAGES ===\n",
    "def extract_images_from_pdf(pdf_path, output_dir, dpi=DPI):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page_num in tqdm(range(len(doc)), desc=f\"[PDFâ†’IMG] {os.path.basename(pdf_path)}\"):\n",
    "            try:\n",
    "                page = doc.load_page(page_num)\n",
    "                pix = page.get_pixmap(dpi=dpi)\n",
    "                out_path = os.path.join(output_dir, f\"page_{page_num+1:04d}.png\")\n",
    "                pix.save(out_path)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed page {page_num+1}: {e}\")\n",
    "    logging.info(f\"Images â†’ {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# === 2) OCR (Paddle PPStructureV3) ===\n",
    "def run_structure_pipeline(pipeline, img_dir, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    image_files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(\".png\")])\n",
    "    for fname in tqdm(image_files, desc=\"[OCR] PPStructureV3\"):\n",
    "        json_path = os.path.join(save_dir, os.path.splitext(fname)[0] + \".json\")\n",
    "        if os.path.exists(json_path):\n",
    "            continue\n",
    "        try:\n",
    "            result = pipeline.predict(input=os.path.join(img_dir, fname))\n",
    "            if result:\n",
    "                result[0].save_to_json(save_path=json_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"OCR error on {fname}: {e}\")\n",
    "\n",
    "# === 3) POSTPROCESS OCR JSON (Corrected and Enhanced) ===\n",
    "def clean_block_content(content):\n",
    "    if not isinstance(content, str):\n",
    "        return content\n",
    "    content = content.replace('\\u221e', 'âˆž')\n",
    "    content = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', content)\n",
    "    content = re.sub(r'\\s{2,}', ' ', content)\n",
    "    return content.strip()\n",
    "\n",
    "def clean_math_expression(expr):\n",
    "    expr = expr.strip()\n",
    "    # General LaTeX cleanup\n",
    "    expr = re.sub(r'\\bexp\\s*\\((.*?)\\)', r'\\\\exp(\\1)', expr)\n",
    "    expr = re.sub(r'\\bsum\\s*\\((.*?)\\)', r'\\\\sum(\\1)', expr)\n",
    "    expr = re.sub(r'\\blog\\s*\\((.*?)\\)', r'\\\\log(\\1)', expr)\n",
    "    expr = re.sub(r'([a-zA-Z])_([a-zA-Z0-9:,]+)', r'\\1_{\\2}', expr)\n",
    "    \n",
    "    # CRITICAL FIX: Heuristic to correct missing 'd\\mathbf{x}' at the end of integrals\n",
    "    # This specifically targets the common OCR error of mistaking 'dx' for 'x'\n",
    "    expr = re.sub(r'(p_{\\\\mathbf{X}}\\(\\\\mathbf{x}\\|\\\\mathcal{C}_[12]\\)\\s*)(\\\\mathbf{x}|\\s*x|)\\s*$', r'\\1d\\\\mathbf{x}', expr) \n",
    "    \n",
    "    return re.sub(r'\\s{2,}', ' ', expr)\n",
    "\n",
    "def group_equations(parsing_blocks):\n",
    "    grouped, buffer = [], []\n",
    "    def flush():\n",
    "        if len(buffer) > 1:\n",
    "            joined_content = \"\\n\".join(b.get(\"block_content\",\"\") for b in buffer)\n",
    "            # Use BBOX that spans min y1 and max y2\n",
    "            all_bboxes = [b[\"block_bbox\"] for b in buffer if \"block_bbox\" in b]\n",
    "            min_x1 = min(b[0] for b in all_bboxes)\n",
    "            min_y1 = min(b[1] for b in all_bboxes)\n",
    "            max_x2 = max(b[2] for b in all_bboxes)\n",
    "            max_y2 = max(b[3] for b in all_bboxes)\n",
    "            group_bbox = [min_x1, min_y1, max_x2, max_y2]\n",
    "\n",
    "            grouped.append({\n",
    "                \"block_label\": \"equation_group\",\n",
    "                \"block_content\": joined_content,\n",
    "                \"block_bbox\": group_bbox, \n",
    "                \"equation_blocks\": buffer.copy()\n",
    "            })\n",
    "        else:\n",
    "            grouped.extend(buffer)\n",
    "        buffer.clear()\n",
    "\n",
    "    for b in parsing_blocks:\n",
    "        if b.get(\"block_label\") in (\"equation\", \"formula\"):\n",
    "            b[\"block_label\"] = \"equation\"\n",
    "            buffer.append(b)\n",
    "        else:\n",
    "            flush()\n",
    "            grouped.append(b)\n",
    "    flush()\n",
    "    return grouped\n",
    "\n",
    "def fix_footnotes(entry):\n",
    "    blocks = entry.get(\"parsing_res_list\", [])\n",
    "    footnotes = [b for b in blocks if b.get(\"block_label\") == \"footnote\"]\n",
    "    for fn in footnotes:\n",
    "        text = clean_block_content(fn.get(\"block_content\", \"\"))\n",
    "        for b in reversed(blocks):\n",
    "            if b.get(\"block_label\") == \"text\":\n",
    "                b[\"block_content\"] = (b.get(\"block_content\",\"\") + \" \" + text).strip()\n",
    "                break\n",
    "        blocks.remove(fn)\n",
    "\n",
    "def postprocess_json_file(src_path, dst_path):\n",
    "    try:\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"[POST] skip {src_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    pages = data if isinstance(data, list) else [data]\n",
    "    for page in pages:\n",
    "        entries = page.get(\"data\", [])\n",
    "        if not entries and \"parsing_res_list\" in page:\n",
    "            entries = [{\"parsing_res_list\": page.get(\"parsing_res_list\", [])}]\n",
    "\n",
    "        for entry in entries:\n",
    "            if \"parsing_res_list\" not in entry:\n",
    "                continue\n",
    "            \n",
    "            for b in entry[\"parsing_res_list\"]:\n",
    "                if \"block_content\" in b:\n",
    "                    b[\"block_content\"] = clean_block_content(b[\"block_content\"])\n",
    "            \n",
    "            fix_footnotes(entry)\n",
    "            \n",
    "            entry[\"parsing_res_list\"] = group_equations(entry[\"parsing_res_list\"])\n",
    "            \n",
    "            for b in entry[\"parsing_res_list\"]:\n",
    "                if b.get(\"block_label\") in (\"equation\", \"equation_group\"):\n",
    "                    # Add cleaned LaTeX field\n",
    "                    b[\"equation_latex\"] = clean_math_expression(b.get(\"block_content\",\"\"))\n",
    "\n",
    "    os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "    with open(dst_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def batch_postprocess(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in tqdm([f for f in os.listdir(input_folder) if f.endswith(\".json\")], desc=\"[POST] Clean JSON\"):\n",
    "        postprocess_json_file(os.path.join(input_folder, fname),\n",
    "                              os.path.join(output_folder, fname))\n",
    "\n",
    "# === 4) YOLO LABELS === (Unchanged)\n",
    "def convert_bbox_to_yolo(bbox, W, H):\n",
    "    x1,y1,x2,y2 = bbox\n",
    "    xc = ((x1+x2)/2)/W\n",
    "    yc = ((y1+y2)/2)/H\n",
    "    w = (x2-x1)/W\n",
    "    h = (y2-y1)/H\n",
    "    return f\"0 {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\"\n",
    "\n",
    "def generate_yolo_labels(cleaned_json_dir, img_dir, label_dir):\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "    count = 0\n",
    "    for fname in tqdm(os.listdir(cleaned_json_dir), desc=\"[LBL] YOLO from JSON\"):\n",
    "        if not fname.endswith(\".json\"): continue\n",
    "        page_id = os.path.splitext(fname)[0]\n",
    "        img_path = os.path.join(img_dir, f\"{page_id}.png\")\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        try:\n",
    "            with open(os.path.join(cleaned_json_dir, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            W,H = Image.open(img_path).size\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[LBL] {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        pages = data if isinstance(data, list) else [data]\n",
    "        yolo_lines = []\n",
    "        for page in pages:\n",
    "            entries = page.get(\"data\", [])\n",
    "            if not entries and \"parsing_res_list\" in page:\n",
    "                entries = [{\"parsing_res_list\": page.get(\"parsing_res_list\", [])}]\n",
    "            for entry in entries:\n",
    "                for b in entry.get(\"parsing_res_list\", []):\n",
    "                    if b.get(\"block_label\") == \"image\":\n",
    "                        bbox = b.get(\"block_bbox\")\n",
    "                        if bbox and len(bbox)==4:\n",
    "                            yolo_lines.append(convert_bbox_to_yolo(bbox,W,H))\n",
    "        if yolo_lines:\n",
    "            with open(os.path.join(label_dir, f\"{page_id}.txt\"), \"w\", encoding=\"utf-8\") as out:\n",
    "                out.write(\"\\n\".join(yolo_lines))\n",
    "            count += 1\n",
    "    logging.info(f\"[LBL] wrote {count} label files â†’ {label_dir}\")\n",
    "\n",
    "# === 5) Split to train/val === (Unchanged)\n",
    "def split_dataset(img_dir, lbl_dir, dataset_dir, train_ratio=TRAIN_RATIO):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    for p in [\"images/train\",\"images/val\",\"labels/train\",\"labels/val\"]:\n",
    "        safe_mkdir(os.path.join(dataset_dir, p))\n",
    "    image_files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(\".png\")])\n",
    "    random.shuffle(image_files)\n",
    "    split_idx = int(len(image_files)*train_ratio)\n",
    "    splits = {\"train\": image_files[:split_idx], \"val\": image_files[split_idx:]}\n",
    "    for split, files in splits.items():\n",
    "        for fname in tqdm(files, desc=f\"[SPLIT] {split}\"):\n",
    "            shutil.copyfile(os.path.join(img_dir,fname), os.path.join(dataset_dir,\"images\",split,fname))\n",
    "            lbl_src = os.path.join(lbl_dir, os.path.splitext(fname)[0]+\".txt\")\n",
    "            if os.path.exists(lbl_src):\n",
    "                shutil.copyfile(lbl_src, os.path.join(dataset_dir,\"labels\",split,os.path.basename(lbl_src)))\n",
    "    logging.info(f\"[SPLIT] dataset ready â†’ {dataset_dir}\")\n",
    "\n",
    "# === MASTER ===\n",
    "def process_books(pdf_folder):\n",
    "    setup_logger()\n",
    "    logging.info(\"Loading PaddleOCR (PPStructureV3)â€¦\")\n",
    "    pipeline = PPStructureV3()\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "    for pdf_file in pdf_files:\n",
    "        book = os.path.splitext(pdf_file)[0]\n",
    "        logging.info(f\"\\n===== BOOK: {book} =====\")\n",
    "        book_dir = safe_mkdir(os.path.join(BASE_OUTPUT, book))\n",
    "        img_dir = safe_mkdir(os.path.join(book_dir, \"images\"))\n",
    "        ocr_dir = safe_mkdir(os.path.join(book_dir, \"paddle_structure_jsons\"))\n",
    "        cleaned_dir = safe_mkdir(os.path.join(book_dir, \"cleaned_semantic_jsons\"))\n",
    "        lbl_dir = safe_mkdir(os.path.join(book_dir, \"labels\"))\n",
    "        dataset_dir = safe_mkdir(os.path.join(book_dir, \"dataset\"))\n",
    "        # 1\n",
    "        extract_images_from_pdf(os.path.join(pdf_folder, pdf_file), img_dir, DPI)\n",
    "        # 2\n",
    "        run_structure_pipeline(pipeline, img_dir, ocr_dir)\n",
    "        # 3\n",
    "        batch_postprocess(ocr_dir, cleaned_dir)\n",
    "        # 4\n",
    "        generate_yolo_labels(cleaned_dir, img_dir, lbl_dir)\n",
    "        # 5\n",
    "        split_dataset(img_dir, lbl_dir, dataset_dir)\n",
    "        logging.info(f\"Finished: {book}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_books(PDF_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fd9dc-1551-4773-bf33-30ace44dc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PART 2/3: Final Corrected Code (Includes Equation Cropping) ===\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "BASE_OUTPUT = r\"C:\\Users\\Admin\\Desktop\\vbooks\"\n",
    "\n",
    "def crop_elements_from_paddle_json(cleaned_json_dir, img_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    total = 0\n",
    "    for fname in os.listdir(cleaned_json_dir):\n",
    "        if not fname.endswith(\".json\"): continue\n",
    "        page_id = os.path.splitext(fname)[0]\n",
    "        image_path = os.path.join(img_dir, f\"{page_id}.png\")\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"[CROP] missing image for {fname}\")\n",
    "            continue\n",
    "        try:\n",
    "            with open(os.path.join(cleaned_json_dir, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            img = Image.open(image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[CROP] {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        pages = data if isinstance(data, list) else [data]\n",
    "        for page in pages:\n",
    "            entries = page.get(\"data\", [])\n",
    "            if not entries and \"parsing_res_list\" in page:\n",
    "                entries = [{\"parsing_res_list\": page.get(\"parsing_res_list\", [])}]\n",
    "            \n",
    "            for entry in entries:\n",
    "                plist = entry.get(\"parsing_res_list\", [])\n",
    "                for block_index, b in enumerate(plist):\n",
    "                    \n",
    "                    block_label = b.get(\"block_label\")\n",
    "                    # Target diagrams, single equations, and grouped equations\n",
    "                    if block_label in (\"image\", \"equation\", \"equation_group\"):\n",
    "                        \n",
    "                        bbox = b.get(\"block_bbox\")\n",
    "                        if not bbox or len(bbox)!=4: continue\n",
    "                        x1,y1,x2,y2 = map(int, bbox)\n",
    "                        if x1>=x2 or y1>=y2: continue\n",
    "                        \n",
    "                        # Define type for filename\n",
    "                        element_type = \"diagram\" if block_label == \"image\" else \"equation\"\n",
    "                        \n",
    "                        crop = img.crop((x1,y1,x2,y2))\n",
    "                        # IMPORTANT: standardized file name for linking\n",
    "                        out_name = f\"{page_id}_{element_type}_{block_index}.png\"\n",
    "                        crop.save(os.path.join(output_dir, out_name))\n",
    "                        total += 1\n",
    "                        \n",
    "    print(f\"[CROP] saved {total} crops â†’ {output_dir}\")\n",
    "\n",
    "def process_labels_and_diagrams(book_dir):\n",
    "    cleaned_json_dir = os.path.join(book_dir, \"cleaned_semantic_jsons\")\n",
    "    img_dir = os.path.join(book_dir, \"images\")\n",
    "    # Output to a single 'assets' folder\n",
    "    assets_dir = os.path.join(book_dir, \"outputs\", \"assets\")\n",
    "    os.makedirs(os.path.join(book_dir, \"outputs\"), exist_ok=True)\n",
    "\n",
    "    if not (os.path.isdir(cleaned_json_dir) and os.path.isdir(img_dir)):\n",
    "        print(f\"[CROP] skip {book_dir}: missing cleaned_semantic_jsons or images\")\n",
    "        return\n",
    "        \n",
    "    crop_elements_from_paddle_json(cleaned_json_dir, img_dir, assets_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Code 2: Element Cropping (Diagrams & Equations) ===\")\n",
    "    for name in os.listdir(BASE_OUTPUT):\n",
    "        d = os.path.join(BASE_OUTPUT, name)\n",
    "        if os.path.isdir(d):\n",
    "            process_labels_and_diagrams(d)\n",
    "    print(\"=== Done ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c497f4-018d-4739-aab3-0f23e2d62749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PART 3/3: Final Corrected Code (Full Semantic Pipeline) ===\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import clip\n",
    "import spacy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "BASE_OUTPUT = r\"C:\\Users\\Admin\\Desktop\\vbooks\"\n",
    "\n",
    "# Initialize CLIP and SpaCy once\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "VALID_BLOCKS = {\"text\",\"paragraph\",\"header\",\"caption\",\"figure_title\"}\n",
    "figure_ref_pattern = re.compile(r\"Figure\\s?(\\d+(?:\\.\\d+)*)\", re.IGNORECASE)\n",
    "equation_ref_pattern = re.compile(r\"(Eq(?:uation)?\\.)\\s*\\((\\d+(?:\\.\\d+)*)\\)\", re.IGNORECASE)\n",
    "\n",
    "LAYOUT_Y_TOL = 250\n",
    "THRESH_SENT2DIAG = 0.22\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def truncate_for_clip(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([t.text for t in doc][:75])\n",
    "\n",
    "def embed_texts(texts):\n",
    "    if not texts: return np.zeros((0,512))\n",
    "    tokens = clip.tokenize([truncate_for_clip(t) for t in texts], truncate=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        return clip_model.encode_text(tokens).cpu().numpy()\n",
    "\n",
    "def embed_images(paths):\n",
    "    embs = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            image = preprocess(Image.open(p).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                embs.append(clip_model.encode_image(image).cpu().numpy()[0])\n",
    "        except:\n",
    "            embs.append(None)\n",
    "    return embs\n",
    "\n",
    "def layout_scores(sentence_bbox, image_bboxes):\n",
    "    if not sentence_bbox: return [0.0]*len(image_bboxes)\n",
    "    sy = (sentence_bbox[1]+sentence_bbox[3])/2\n",
    "    # Simple score based on vertical inverse distance\n",
    "    return [1/(1+abs(sy - ((b[1]+b[3])/2))) for b in image_bboxes]\n",
    "\n",
    "def hybrid_match(sentence_emb, diagram_embs, layout_sc):\n",
    "    # Combines semantic score (85%) and layout score (15%)\n",
    "    cs = cosine_similarity([sentence_emb], diagram_embs)[0]\n",
    "    return np.argmax([0.85*c + 0.15*layout_sc[i] for i,c in enumerate(cs)])\n",
    "\n",
    "def load_blocks(cleaned_json):\n",
    "    # returns list of blocks for single page (flat)\n",
    "    data = cleaned_json\n",
    "    if isinstance(data, list):\n",
    "        plist = data[0].get(\"parsing_res_list\", []) if data else []\n",
    "    elif isinstance(data, dict):\n",
    "        if \"data\" in data and isinstance(data[\"data\"], list) and data[\"data\"]:\n",
    "            plist = data[\"data\"][0].get(\"parsing_res_list\", [])\n",
    "        else:\n",
    "            plist = data.get(\"parsing_res_list\", [])\n",
    "    else:\n",
    "        plist = []\n",
    "    return plist\n",
    "\n",
    "def extract_sentences(cleaned_json_dir, sentence_dir):\n",
    "    os.makedirs(sentence_dir, exist_ok=True)\n",
    "    for fname in os.listdir(cleaned_json_dir):\n",
    "        if not fname.endswith(\".json\"): continue\n",
    "        page_id = fname[:-5]\n",
    "        with open(os.path.join(cleaned_json_dir, fname),\"r\",encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        blocks = load_blocks(data)\n",
    "\n",
    "        out, sid = [], 0\n",
    "        for bi, b in enumerate(blocks):\n",
    "            if b.get(\"block_label\",\"\") not in VALID_BLOCKS: continue\n",
    "            text = (b.get(\"block_content\") or \"\").strip()\n",
    "            if not text: continue\n",
    "            bbox = b.get(\"block_bbox\")\n",
    "            for sidx, sent in enumerate(nlp(text).sents):\n",
    "                t = sent.text.strip()\n",
    "                if len(t) < 5: continue\n",
    "                out.append({\n",
    "                    \"text\": t, \"sentence_id\": sid, \"block_index\": bi,\n",
    "                    \"block_label\": b.get(\"block_label\"), \"sentence_index\": sidx,\n",
    "                    \"bbox\": bbox\n",
    "                })\n",
    "                sid += 1\n",
    "        with open(os.path.join(sentence_dir, f\"{page_id}.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump({\"sentences\": out}, f, indent=2)\n",
    "    print(f\"[SENTS] â†’ {sentence_dir}\")\n",
    "\n",
    "def map_formula_numbers(blocks):\n",
    "    \"\"\"Return dict: '1.27' -> index_of_nearest_equation\"\"\"\n",
    "    eq_idxs = [(i,b) for i,b in enumerate(blocks) if b.get(\"block_label\") in (\"equation\",\"equation_group\",\"formula\")]\n",
    "    nums = [(i,b) for i,b in enumerate(blocks) if b.get(\"block_label\") in (\"formula_number\",\"equation_number\")]\n",
    "    mapping = {}\n",
    "    def ycenter(bb): return (bb[1]+bb[3])/2 if bb else 0\n",
    "    for i_num, b_num in nums:\n",
    "        num_txt = (b_num.get(\"block_content\") or \"\").strip()\n",
    "        m = re.search(r\"\\((\\d+(?:\\.\\d+)*)\\)\", num_txt)\n",
    "        if not m: continue\n",
    "        y_num = ycenter(b_num.get(\"block_bbox\"))\n",
    "        closest = None; best = 1e9\n",
    "        for i_eq, b_eq in eq_idxs:\n",
    "            y_eq = ycenter(b_eq.get(\"block_bbox\"))\n",
    "            d = abs(y_eq - y_num)\n",
    "            if d < best:\n",
    "                best, closest = d, i_eq\n",
    "        if closest is not None:\n",
    "            mapping[m.group(1)] = closest\n",
    "    return mapping\n",
    "\n",
    "def tokens_from_latex(s):\n",
    "    # more permissive: include one-letter tokens (x,p,R), keep greek words\n",
    "    return set([t.lower() for t in re.findall(r\"[A-Za-z]+\", s)])\n",
    "\n",
    "# --- MAIN LINKING FUNCTION ---\n",
    "def semantic_linking(sentence_dir, cleaned_json_dir, assets_dir, links_dir):\n",
    "    os.makedirs(links_dir, exist_ok=True)\n",
    "    \n",
    "    # Filter for diagram assets only\n",
    "    diag_asset_files_all = sorted([f for f in os.listdir(assets_dir) if \"diagram\" in f and f.endswith(\".png\")])\n",
    "    \n",
    "    for fname in tqdm(sorted(os.listdir(sentence_dir)), desc=\"[LINK]\"):\n",
    "        if not fname.endswith(\".json\"): continue\n",
    "        page_id = fname[:-5]\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(sentence_dir, fname),\"r\",encoding=\"utf-8\") as f:\n",
    "                sent_data = json.load(f)\n",
    "            with open(os.path.join(cleaned_json_dir, f\"{page_id}.json\"),\"r\",encoding=\"utf-8\") as f:\n",
    "                page_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[LINK] skip {page_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        sentences = sent_data.get(\"sentences\", [])\n",
    "        s_texts = [s[\"text\"] for s in sentences]\n",
    "        s_embs = embed_texts(s_texts) if len(s_texts) else np.zeros((0,512))\n",
    "\n",
    "        blocks = load_blocks(page_data)\n",
    "        figures = [(b.get(\"block_content\"), b.get(\"block_bbox\")) for b in blocks if b.get(\"block_label\")==\"figure_title\"]\n",
    "        \n",
    "        # --- Diagram Linking ---\n",
    "        # Filter diagram assets for the current page\n",
    "        diag_files_all = [f for f in diag_asset_files_all if f.startswith(page_id)]\n",
    "        diag_paths = [os.path.join(assets_dir,f) for f in diag_files_all]\n",
    "        diag_embs_all = embed_images(diag_paths)\n",
    "        valid = []\n",
    "        for fimg, emb in zip(diag_files_all, diag_embs_all):\n",
    "            try:\n",
    "                # Extract block index from filename: {page_id}_diagram_{block_index}.png\n",
    "                bidx = int(fimg.split(\"_\")[-1].split(\".\")[0])\n",
    "            except:\n",
    "                continue\n",
    "            if emb is None: continue\n",
    "            if bidx < len(blocks) and blocks[bidx].get(\"block_label\")==\"image\":\n",
    "                valid.append((fimg, bidx, emb, blocks[bidx].get(\"block_bbox\")))\n",
    "        sent2diag, caption2diag, textref2diag = [], [], []\n",
    "\n",
    "        if valid and len(sentences):\n",
    "            files, idxs, emb_list, vbboxes = zip(*valid)\n",
    "            \n",
    "            # Sentence-to-Diagram Semantic Match\n",
    "            for i,(s_obj, s_emb) in enumerate(zip(sentences, s_embs)):\n",
    "                ls = layout_scores(s_obj.get(\"bbox\"), list(vbboxes))\n",
    "                best = hybrid_match(s_emb, list(emb_list), ls)\n",
    "                cs = cosine_similarity([s_emb], list(emb_list))[0]\n",
    "                comb = 0.85*cs[best] + 0.15*ls[best]\n",
    "                if comb > THRESH_SENT2DIAG:\n",
    "                    sent2diag.append({\n",
    "                        \"sentence\": s_obj[\"text\"], \"sentence_id\": i, \"bbox\": s_obj.get(\"bbox\"),\n",
    "                        \"diagram_file\": files[best], \"score\": float(round(comb,4))\n",
    "                    })\n",
    "\n",
    "            # Caption-to-Diagram (Layout Match)\n",
    "            for cap, cap_box in figures:\n",
    "                if not cap_box: continue\n",
    "                cy = (cap_box[1]+cap_box[3])/2\n",
    "                closest = min(valid, key=lambda v: abs(((v[3][1]+v[3][3])/2) - cy))\n",
    "                caption2diag.append({\n",
    "                    \"caption\": cap, \"bbox\": cap_box, \"diagram_file\": closest[0], \"score\": 0.9\n",
    "                })\n",
    "\n",
    "            # Textual Reference-to-Diagram\n",
    "            for i, s_obj in enumerate(sentences):\n",
    "                m = figure_ref_pattern.search(s_obj[\"text\"])\n",
    "                if m:\n",
    "                    num = m.group(1)\n",
    "                    for cap in caption2diag:\n",
    "                        if num in (cap.get(\"caption\") or \"\"):\n",
    "                            textref2diag.append({\n",
    "                                \"sentence\": s_obj[\"text\"], \"sentence_id\": i, \"bbox\": s_obj.get(\"bbox\"),\n",
    "                                \"diagram_file\": cap[\"diagram_file\"], \"score\": 0.8\n",
    "                            })\n",
    "                            break\n",
    "        \n",
    "        # --- Equation Linking ---\n",
    "        num_to_eqidx = map_formula_numbers(blocks)\n",
    "        equations = [(i,b) for i,b in enumerate(blocks) if b.get(\"block_label\") in (\"equation\",\"equation_group\",\"formula\")]\n",
    "        sent2eq = []\n",
    "\n",
    "        for s in sentences:\n",
    "            s_text = s[\"text\"]; s_bbox = s.get(\"bbox\")\n",
    "            matched = False\n",
    "            \n",
    "            # 1) explicit Eq.(x.x) reference\n",
    "            m = equation_ref_pattern.search(s_text)\n",
    "            if m:\n",
    "                eq_no = m.group(2)\n",
    "                if eq_no in num_to_eqidx:\n",
    "                    i_eq = num_to_eqidx[eq_no]\n",
    "                    b_eq = blocks[i_eq]\n",
    "                    sent2eq.append({\n",
    "                        \"sentence\": s_text, \"sentence_id\": s[\"sentence_id\"], \"bbox\": s_bbox,\n",
    "                        \"equation_index\": i_eq, \n",
    "                        \"equation_latex\": b_eq.get(\"equation_latex\"), # <-- Use cleaned LaTeX\n",
    "                        \"equation_bbox\": b_eq.get(\"block_bbox\"), \"match_type\": \"eq_number_ref\"\n",
    "                    })\n",
    "                    matched = True\n",
    "\n",
    "            if matched: continue\n",
    "\n",
    "            # 2) token overlap with LaTeX\n",
    "            s_tokens = tokens_from_latex(s_text)\n",
    "            best_tok = None\n",
    "            for i_eq, b_eq in equations:\n",
    "                e_tokens = tokens_from_latex(b_eq.get(\"equation_latex\") or \"\") # <-- Use cleaned LaTeX\n",
    "                if s_tokens & e_tokens:\n",
    "                    best_tok = (i_eq, b_eq); break\n",
    "            if best_tok:\n",
    "                i_eq, b_eq = best_tok\n",
    "                sent2eq.append({\n",
    "                    \"sentence\": s_text, \"sentence_id\": s[\"sentence_id\"], \"bbox\": s_bbox,\n",
    "                    \"equation_index\": i_eq, \n",
    "                    \"equation_latex\": b_eq.get(\"equation_latex\"), # <-- Use cleaned LaTeX\n",
    "                    \"equation_bbox\": b_eq.get(\"block_bbox\"), \"match_type\": \"token_overlap\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # 3) layout proximity\n",
    "            if s_bbox:\n",
    "                sy = (s_bbox[1]+s_bbox[3])/2\n",
    "                closest = None; bestd = 1e9\n",
    "                for i_eq, b_eq in equations:\n",
    "                    eb = b_eq.get(\"block_bbox\")\n",
    "                    if not eb: continue\n",
    "                    ey = (eb[1]+eb[3])/2\n",
    "                    d = abs(sy - ey)\n",
    "                    if d < bestd:\n",
    "                        bestd, closest = d, (i_eq, b_eq)\n",
    "                if closest and bestd < LAYOUT_Y_TOL:\n",
    "                    i_eq, b_eq = closest\n",
    "                    sent2eq.append({\n",
    "                        \"sentence\": s_text, \"sentence_id\": s[\"sentence_id\"], \"bbox\": s_bbox,\n",
    "                        \"equation_index\": i_eq, \n",
    "                        \"equation_latex\": b_eq.get(\"equation_latex\"), # <-- Use cleaned LaTeX\n",
    "                        \"equation_bbox\": b_eq.get(\"block_bbox\"), \"match_type\": \"layout_near\"\n",
    "                    })\n",
    "\n",
    "        with open(os.path.join(links_dir, f\"{page_id}.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"page\": page_id,\n",
    "                \"sentence_to_diagram\": sent2diag,\n",
    "                \"caption_to_diagram\": caption2diag,\n",
    "                \"textual_reference\": textref2diag,\n",
    "                \"sentence_to_equation\": sent2eq\n",
    "            }, f, indent=2)\n",
    "        print(f\"[LINK] {page_id} â†’ S2D {len(sent2diag)} | C2D {len(caption2diag)} | TRef {len(textref2diag)} | S2E {len(sent2eq)}\")\n",
    "\n",
    "# --- MERGE FUNCTION ---\n",
    "def merge_semantic_pages(sentence_dir, cleaned_json_dir, links_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for fname in tqdm(sorted(os.listdir(sentence_dir)), desc=\"[MERGE]\"):\n",
    "        if not fname.endswith(\".json\"): continue\n",
    "        page_id = fname[:-5]\n",
    "        try:\n",
    "            with open(os.path.join(sentence_dir, fname),\"r\",encoding=\"utf-8\") as f:\n",
    "                sdata = json.load(f)\n",
    "            with open(os.path.join(cleaned_json_dir, f\"{page_id}.json\"),\"r\",encoding=\"utf-8\") as f:\n",
    "                raw = json.load(f)\n",
    "            with open(os.path.join(links_dir, f\"{page_id}.json\"),\"r\",encoding=\"utf-8\") as f:\n",
    "                ldata = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[MERGE] missing file for {page_id}, skip\")\n",
    "            continue\n",
    "        blocks = load_blocks(raw)\n",
    "        merged = {\n",
    "            \"page_id\": page_id,\n",
    "            \"sentences\": sdata.get(\"sentences\", []),\n",
    "            \"blocks\": blocks,\n",
    "            \"links\": {\n",
    "                \"sentence_to_diagram\": ldata.get(\"sentence_to_diagram\", []),\n",
    "                \"caption_to_diagram\": ldata.get(\"caption_to_diagram\", []),\n",
    "                \"textual_reference\": ldata.get(\"textual_reference\", []),\n",
    "                \"sentence_to_equation\": ldata.get(\"sentence_to_equation\", [])\n",
    "            }\n",
    "        }\n",
    "        with open(os.path.join(output_dir, f\"{page_id}.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(merged, f, indent=2)\n",
    "    print(f\"[MERGE] â†’ {output_dir}\")\n",
    "\n",
    "# --- MASTER EXECUTION FOR PART 3 ---\n",
    "def process_semantic_pipeline(book_dir):\n",
    "    print(f\"\\n=== Semantic Pipeline: {book_dir} ===\")\n",
    "    cleaned_json_dir = os.path.join(book_dir, \"cleaned_semantic_jsons\")\n",
    "    sentence_dir = os.path.join(book_dir, \"outputs\", \"sm_sentences\")\n",
    "    links_dir = os.path.join(book_dir, \"outputs\", \"semantic_links\")\n",
    "    assets_dir = os.path.join(book_dir, \"outputs\", \"assets\")\n",
    "    merged_dir = os.path.join(book_dir, \"outputs\", \"semantic_pages\")\n",
    "    for p in [os.path.join(book_dir,\"outputs\"), sentence_dir, links_dir, merged_dir]:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "    # 8) sentences\n",
    "    extract_sentences(cleaned_json_dir, sentence_dir)\n",
    "    # 9) links\n",
    "    semantic_linking(sentence_dir, cleaned_json_dir, assets_dir, links_dir)\n",
    "    # 10) merge\n",
    "    merge_semantic_pages(sentence_dir, cleaned_json_dir, links_dir, merged_dir)\n",
    "    print(f\"=== Done: {book_dir} ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for name in os.listdir(BASE_OUTPUT):\n",
    "        d = os.path.join(BASE_OUTPUT, name)\n",
    "        if os.path.isdir(d):\n",
    "            # NOTE: Before running, ensure Part 1 and Part 2 have completed successfully.\n",
    "            process_semantic_pipeline(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ee7a7-73c2-4cdb-ac70-01fb3489630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELL 1 â€” CONFIG + UTILITIES\n",
    "# ================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import pathlib\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ðŸ”§ MODEL NAMES (YOUR MODELS)\n",
    "# ------------------------------------------------\n",
    "VISION_MODEL = \"qwen3-vl:8b\"     # vision-language model\n",
    "TEXT_MODEL   = \"llama3:8b\"       # text-only reasoning modelAA\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "RETRY_COUNT = 3\n",
    "RETRY_DELAY = 3\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ðŸ” PATH + BASE64 HELPERS\n",
    "# ------------------------------------------------\n",
    "def to_posix(p: str) -> str:\n",
    "    \"\"\"Windows path â†’ POSIX (required for training jsonl).\"\"\"\n",
    "    return pathlib.Path(p).as_posix()\n",
    "\n",
    "def load_image_b64(path: str) -> Optional[str]:\n",
    "    \"\"\"Read an image file and return base64 string.\"\"\"\n",
    "    if not path or not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ðŸ§  ROBUST JSON EXTRACTOR (handles messy output)\n",
    "# ------------------------------------------------\n",
    "def extract_json_raw(text: str) -> Any:\n",
    "    \"\"\"\n",
    "    Find JSON between <JSON>...</JSON> if present.\n",
    "    Otherwise try to auto-detect the first valid JSON block.\n",
    "    \"\"\"\n",
    "    # Look for <JSON> ... </JSON>\n",
    "    block = re.search(r\"<JSON>([\\s\\S]+?)</JSON>\", text)\n",
    "    if block:\n",
    "        try:\n",
    "            return json.loads(block.group(1).strip())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Fallback: find first {...} or [...]\n",
    "    def find_balanced(s, oc, cc):\n",
    "        depth = 0\n",
    "        start = None\n",
    "        for i, ch in enumerate(s):\n",
    "            if ch == oc:\n",
    "                if start is None:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == cc and depth > 0:\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    return s[start:i+1]\n",
    "        return None\n",
    "\n",
    "    for oc, cc in [(\"{\", \"}\"), (\"[\", \"]\")]:\n",
    "        candidate = find_balanced(text, oc, cc)\n",
    "        if candidate:\n",
    "            try:\n",
    "                return json.loads(candidate)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Nothing worked â†’ return empty\n",
    "    return {}\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ðŸš€ UNIVERSAL OLLAMA CALL\n",
    "# ------------------------------------------------\n",
    "def ollama_call(prompt: str, model: str, images: Optional[List[str]] = None,\n",
    "                max_tokens: int = 120) -> str:\n",
    "    \"\"\"\n",
    "    Universal caller for both Qwen-VL and LLaMA.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"num_predict\": max_tokens\n",
    "    }\n",
    "    if images:\n",
    "        payload[\"images\"] = [img for img in images if img]\n",
    "\n",
    "    for attempt in range(RETRY_COUNT):\n",
    "        try:\n",
    "            r = requests.post(OLLAMA_URL, json=payload, timeout=240)\n",
    "            if r.status_code != 200:\n",
    "                raise RuntimeError(r.text[:300])\n",
    "            out = r.json()\n",
    "            return out.get(\"response\", json.dumps(out))\n",
    "        except Exception as e:\n",
    "            if attempt < RETRY_COUNT - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Ollama FAILED: {e}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ðŸ“‘ PAGE TYPE DETECTOR\n",
    "# ------------------------------------------------\n",
    "def detect_page_type(context: str, has_image: bool) -> str:\n",
    "    \"\"\"\n",
    "    Decide mode:\n",
    "      - 'image'      â†’ diagram/figure detected\n",
    "      - 'equation'   â†’ equation-heavy text pages\n",
    "      - 'text'       â†’ plain text pages\n",
    "    \"\"\"\n",
    "    \n",
    "    if has_image:\n",
    "        return \"image\"\n",
    "\n",
    "    # equation detection\n",
    "    eq_patterns = [\n",
    "        r\"\\\\begin\\{equation\\}\",\n",
    "        r\"\\\\frac\",\n",
    "        r\"\\\\sum\",\n",
    "        r\"\\\\int\",\n",
    "        r\"=\",\n",
    "    ]\n",
    "    eq_count = sum(bool(re.search(p, context)) for p in eq_patterns)\n",
    "\n",
    "    if eq_count >= 2:\n",
    "        return \"equation\"\n",
    "\n",
    "    return \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264941f-038d-4471-ab1c-0e0b6f0b5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# CELL 2 â€” MULTI-MODEL AGENTS\n",
    "# ================================================\n",
    "\n",
    "import random\n",
    "\n",
    "class TextbookTrainingAgents:\n",
    "    def __init__(self):\n",
    "        self.vision_model = VISION_MODEL\n",
    "        self.text_model   = TEXT_MODEL\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # QUESTION GENERATION (image â†’ Qwen; text â†’ LLaMA)\n",
    "    # ------------------------------------------------------------\n",
    "    def generate_questions(self, context: str, page_type: str,\n",
    "                           b64: Optional[str]) -> List[str]:\n",
    "\n",
    "        if page_type == \"image\":\n",
    "            model = self.vision_model\n",
    "            img = [b64] if b64 else None\n",
    "        else:\n",
    "            model = self.text_model\n",
    "            img = None\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "SYSTEM:\n",
    "Generate 3â€“5 short questions (max 18 words).\n",
    "Rules:\n",
    "- Must be answerable ONLY from the provided TEXT and (if present) IMAGE.\n",
    "- If an image is included, the question MUST reference or depend on the image.\n",
    "- Avoid LaTeX in the question text.\n",
    "- Output ONLY a JSON list inside <JSON>...</JSON>.\n",
    "\n",
    "TEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "        if page_type == \"image\":\n",
    "            prompt += \"\\nIMAGE: <image provided>\\n\"\n",
    "\n",
    "        try:\n",
    "            resp = ollama_call(prompt, model, img, max_tokens=120)\n",
    "            data = extract_json_raw(resp)\n",
    "            if isinstance(data, list) and data:\n",
    "                return [str(x).strip() for x in data]\n",
    "            return [\"Explain this content.\"]\n",
    "        except:\n",
    "            return [\"Explain this content.\"]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STUDENT ANSWER (image â†’ Qwen; text â†’ LLaMA)\n",
    "    # ------------------------------------------------------------\n",
    "    def student_answer(self, question: str, context: str,\n",
    "                       page_type: str, b64: Optional[str]) -> str:\n",
    "\n",
    "        if page_type == \"image\":\n",
    "            model = self.vision_model\n",
    "            img = [b64] if b64 else None\n",
    "        else:\n",
    "            model = self.text_model\n",
    "            img = None\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "SYSTEM:\n",
    "You are a BEGINNER-LEVEL tutor.\n",
    "Rules:\n",
    "- Use ONLY the textbook TEXT (and IMAGE if provided).\n",
    "- No outside knowledge.\n",
    "- If the answer is missing, say:\n",
    "  \"The textbook context does not provide this information.\"\n",
    "- 2â€“4 clear sentences.\n",
    "- No markdown.\n",
    "- End with [p.?].\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "TEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "        if page_type == \"image\":\n",
    "            prompt += \"\\nIMAGE: <image provided>\\n\"\n",
    "\n",
    "        return ollama_call(prompt, model, img, max_tokens=150)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # EXPERT ANSWER (image â†’ Qwen; text â†’ LLaMA)\n",
    "    # ------------------------------------------------------------\n",
    "    def expert_answer(self, question: str, context: str,\n",
    "                      page_type: str, b64: Optional[str]) -> str:\n",
    "\n",
    "        if page_type == \"image\":\n",
    "            model = self.vision_model\n",
    "            img = [b64] if b64 else None\n",
    "        else:\n",
    "            model = self.text_model\n",
    "            img = None\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "SYSTEM:\n",
    "You are an EXPERT-LEVEL tutor strictly limited to the TEXTBOOK CONTEXT.\n",
    "Rules:\n",
    "-Use ONLY the TEXT (and IMAGE if available).\n",
    "- DO NOT introduce equations or symbols not present in the context.\n",
    "- LaTeX is allowed, but ONLY for equations that appear in the context.\n",
    "- Provide a clear explanation using BOTH:\n",
    "    â€¢ LaTeX (when present in context)\n",
    "    â€¢ Verbal description of the math\n",
    "- Provide 4â€“12 sentences depending on complexity.\n",
    "- If the derivation steps are NOT shown in the context, say so.\n",
    "- No markdown formatting.\n",
    "- DO NOT use outside facts.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "TEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "        if page_type == \"image\":\n",
    "            prompt += \"\\nIMAGE: <image provided>\\n\"\n",
    "\n",
    "        return ollama_call(prompt, model, img, max_tokens=220)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # VALIDATOR (always uses LLaMA â€” fastest)\n",
    "    # ------------------------------------------------------------\n",
    "    def validate(self, question: str, student: str, expert: str,\n",
    "                 context: str, b64: Optional[str]) -> Dict[str, Any]:\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "SYSTEM:\n",
    "Soft validator.\n",
    "- Approve by default.\n",
    "- Reject ONLY if the answer contradicts the TEXT.\n",
    "- LaTeX allowed.\n",
    "- Output JSON inside <JSON>...</JSON>.\n",
    "\n",
    "QUESTION: {question}\n",
    "STUDENT: {student}\n",
    "EXPERT: {expert}\n",
    "TEXT: {context}\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            resp = ollama_call(prompt, self.text_model, None, max_tokens=120)\n",
    "            data = extract_json_raw(resp)\n",
    "\n",
    "            return {\n",
    "                \"student_approved\": bool(data.get(\"student_approved\", True)),\n",
    "                \"expert_approved\":  bool(data.get(\"expert_approved\",  True)),\n",
    "                \"student_issues\":   data.get(\"student_issues\", []),\n",
    "                \"expert_issues\":    data.get(\"expert_issues\", [])\n",
    "            }\n",
    "        except:\n",
    "            # fallback: approve both\n",
    "            return {\n",
    "                \"student_approved\": True,\n",
    "                \"expert_approved\": True,\n",
    "                \"student_issues\": [],\n",
    "                \"expert_issues\": []\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eebe59a-0bb8-4f45-b9ef-e4c200c424b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# CELL 3 â€” PAGE-LEVEL HYBRID GENERATOR (FULL POWER)\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class HybridLlavaGenerator:\n",
    "    \"\"\"\n",
    "    One page -> one big context -> 3â€“5 Qs -> student + expert answers.\n",
    "    Uses:\n",
    "      - Qwen-VL for image pages\n",
    "      - LLaMA3 for pure-text / equation pages\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agents: TextbookTrainingAgents):\n",
    "        self.agents = agents\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # checkpoint helpers\n",
    "    # --------------------------------------------------------------\n",
    "    def load_checkpoint(self, ckpt_path: str):\n",
    "        if not os.path.exists(ckpt_path):\n",
    "            return []\n",
    "        buf = []\n",
    "        with open(ckpt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    buf.append(json.loads(line))\n",
    "                except:\n",
    "                    pass\n",
    "        print(f\"ðŸ”„ Loaded checkpoint ({len(buf)} items)\")\n",
    "        return buf\n",
    "\n",
    "    def save_checkpoint(self, data, ckpt_path: str):\n",
    "        with open(ckpt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"ðŸ’¾ Saved checkpoint ({len(data)} items)\")\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # main book processor  (PAGE-LEVEL)\n",
    "    # --------------------------------------------------------------\n",
    "    def process_book(self, semantic_dir: str, assets_dir: str,\n",
    "                     output_path: str, ckpt_path: str, max_pages: int):\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # load previous stuff\n",
    "        training_data = self.load_checkpoint(ckpt_path)\n",
    "        done_ids = {x[\"id\"] for x in training_data}\n",
    "\n",
    "        # which pages already have *any* sample?\n",
    "        done_pages = set()\n",
    "        for x in training_data:\n",
    "            pid = str(x[\"id\"]).split(\"_\")[0]\n",
    "            done_pages.add(pid)\n",
    "\n",
    "        files = sorted([f for f in os.listdir(semantic_dir) if f.endswith(\".json\")])\n",
    "        if max_pages:\n",
    "            files = files[:max_pages]\n",
    "\n",
    "        print(f\"\\nðŸ“˜ PAGE-LEVEL processing for {len(files)} pages\\n\")\n",
    "\n",
    "        for fname in tqdm(files):\n",
    "            page_id = os.path.splitext(fname)[0]\n",
    "\n",
    "            # skip pages we already processed in a previous run\n",
    "            if page_id in done_pages:\n",
    "                continue\n",
    "\n",
    "            page_path = os.path.join(semantic_dir, fname)\n",
    "            try:\n",
    "                with open(page_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    page_data = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to read {fname}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # always set page_id on the JSON (for context string)\n",
    "            page_data[\"page_id\"] = page_id\n",
    "\n",
    "            # ------------- build page-level context -------------\n",
    "            context, eq_list = self.build_page_context(page_data)\n",
    "\n",
    "            # ------------- pick an image (diagram if exists) ----\n",
    "            img_path = self.pick_page_image(page_data, assets_dir)\n",
    "            b64 = load_image_b64(img_path) if img_path else None\n",
    "            has_image = b64 is not None\n",
    "\n",
    "            page_type = detect_page_type(context, has_image)\n",
    "\n",
    "            # ------------- generate questions for this page -----\n",
    "            try:\n",
    "                questions = self.agents.generate_questions(context, page_type, b64)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Question gen failed for {page_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if not questions:\n",
    "                continue\n",
    "\n",
    "            # cap questions per page (3â€“5 typical)\n",
    "            questions = questions[:5]\n",
    "\n",
    "            # ------------- run student + expert for each Q ------\n",
    "            for q_idx, q in enumerate(questions):\n",
    "                # simple safe id\n",
    "                raw = re.sub(r\"\\s+\", \"_\", q)\n",
    "                raw = re.sub(r\"[^A-Za-z0-9_-]\", \"\", raw)\n",
    "                safe = raw[:40] if raw else f\"q{q_idx}\"\n",
    "                base_id = f\"{page_id}_{safe}\"\n",
    "\n",
    "                sid = f\"{base_id}_s\"\n",
    "                eid = f\"{base_id}_e\"\n",
    "\n",
    "                if sid in done_ids and eid in done_ids:\n",
    "                    continue\n",
    "\n",
    "                # answers\n",
    "                try:\n",
    "                    stu_ans = self.agents.student_answer(q, context, page_type, b64)\n",
    "                    exp_ans = self.agents.expert_answer(q, context, page_type, b64)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Answer gen failed ({page_id} / '{q[:30]}'): {e}\")\n",
    "                    continue\n",
    "\n",
    "                # validation (LLaMA)\n",
    "                val = self.agents.validate(q, stu_ans, exp_ans, context, b64)\n",
    "                stu_ok = val.get(\"student_approved\", True)\n",
    "                exp_ok = val.get(\"expert_approved\", True)\n",
    "\n",
    "                # student sample\n",
    "                if stu_ok and sid not in done_ids:\n",
    "                    training_data.append({\n",
    "                        \"id\": sid,\n",
    "                        \"image\": to_posix(img_path) if img_path else None,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"human\",\n",
    "                                \"value\": f\"<image>\\n{q}\" if has_image else q\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"gpt\",\n",
    "                                \"value\": stu_ans\n",
    "                            }\n",
    "                        ],\n",
    "                        \"level\": \"student\"\n",
    "                    })\n",
    "                    done_ids.add(sid)\n",
    "\n",
    "                # expert sample\n",
    "                if exp_ok and eid not in done_ids:\n",
    "                    training_data.append({\n",
    "                        \"id\": eid,\n",
    "                        \"image\": to_posix(img_path) if img_path else None,\n",
    "                        \"conversations\": [\n",
    "                            {\n",
    "                                \"from\": \"human\",\n",
    "                                \"value\": f\"<image>\\n{q}\" if has_image else q\n",
    "                            },\n",
    "                            {\n",
    "                                \"from\": \"gpt\",\n",
    "                                \"value\": exp_ans\n",
    "                            }\n",
    "                        ],\n",
    "                        \"level\": \"expert\"\n",
    "                    })\n",
    "                    done_ids.add(eid)\n",
    "\n",
    "            # mark this page as processed and checkpoint\n",
    "            done_pages.add(page_id)\n",
    "            self.save_checkpoint(training_data, ckpt_path)\n",
    "\n",
    "        # ------------------ final save -------------------------\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in training_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"\\nâœ… PAGE-LEVEL DONE â€” {len(training_data)} samples â†’ {output_path}\")\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # build PAGE-LEVEL context (text + equations)\n",
    "    # --------------------------------------------------------------\n",
    "    def build_page_context(self, page_data: Dict[str, Any]) -> (str, List[str]):\n",
    "        sentences = page_data.get(\"sentences\", [])\n",
    "        txt_bits = []\n",
    "\n",
    "        for s in sentences:\n",
    "            t = s.get(\"text\", \"\").strip()\n",
    "            if t:\n",
    "                txt_bits.append(t)\n",
    "\n",
    "        raw_text = \" \".join(txt_bits)\n",
    "\n",
    "        # gather unique equations from links (page-level)\n",
    "        eqs = []\n",
    "        seen = set()\n",
    "        for link in page_data.get(\"links\", {}).get(\"sentence_to_equation\", []):\n",
    "            eq = link.get(\"equation_latex\", \"\")\n",
    "            if eq and eq not in seen:\n",
    "                seen.add(eq)\n",
    "                eqs.append(eq)\n",
    "\n",
    "        eq_block = \"\"\n",
    "        if eqs:\n",
    "            # keep them short-ish\n",
    "            eq_lines = [f\"[Eq {i+1}] {e}\" for i, e in enumerate(eqs)]\n",
    "            eq_block = \"\\nEQUATIONS ON THIS PAGE:\\n\" + \"\\n\".join(eq_lines)\n",
    "\n",
    "        page_id = page_data.get(\"page_id\", \"\")\n",
    "        context = raw_text\n",
    "        if eq_block:\n",
    "            context += \"\\n\" + eq_block\n",
    "        context += f\"\\n[Source: {page_id}]\"\n",
    "\n",
    "        # safety: trim insane length\n",
    "        context = context[:4000]\n",
    "\n",
    "        return context, eqs\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # choose ONE image for the page (diagram preferred)\n",
    "    # --------------------------------------------------------------\n",
    "    def pick_page_image(self, page_data: Dict[str, Any], assets_dir: str) -> Optional[str]:\n",
    "        links = page_data.get(\"links\", {})\n",
    "\n",
    "        # 1) prefer sentence_to_diagram links\n",
    "        diag_links = links.get(\"sentence_to_diagram\", []) or []\n",
    "        for dl in diag_links:\n",
    "            fname = dl.get(\"diagram_file\")\n",
    "            if not fname:\n",
    "                continue\n",
    "            full = os.path.join(assets_dir, fname)\n",
    "            if os.path.exists(full):\n",
    "                return full\n",
    "\n",
    "        # 2) fallback: anything in assets_dir starting with page_id_\n",
    "        page_id = page_data.get(\"page_id\", \"\")\n",
    "        if os.path.isdir(assets_dir):\n",
    "            for f in os.listdir(assets_dir):\n",
    "                if f.startswith(page_id) and f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
    "                    return os.path.join(assets_dir, f)\n",
    "\n",
    "        # 3) no image for this page\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0721ded-88bc-49d4-8722-91d8a42ea544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# CELL 4 â€” RUN THE HYBRID GENERATOR\n",
    "# ==========================================================\n",
    "\n",
    "# Create the generator using agents defined in Cell 2\n",
    "agents = TextbookTrainingAgents()\n",
    "runner = HybridLlavaGenerator(agents)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURE YOUR PATHS HERE\n",
    "# ----------------------------------------------------------\n",
    "BASE = \"C:/Users/Admin/Desktop/vbooks\"   # change if needed\n",
    "\n",
    "# Books inside BASE folder (each should have /outputs/semantic_pages & /outputs/assets)\n",
    "books = [b for b in os.listdir(BASE) if os.path.isdir(os.path.join(BASE, b))]\n",
    "\n",
    "print(\"ðŸ“š Books detected:\", books)\n",
    "\n",
    "# SAMPLE mode?\n",
    "sample = input(\"Run SAMPLE mode (first 3 pages)? (y/n): \").lower().strip() == \"y\"\n",
    "max_pages = 3 if sample else 9999\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# RUN THROUGH ALL BOOKS\n",
    "# ----------------------------------------------------------\n",
    "for book in books:\n",
    "    print(f\"\\nðŸ“˜ Processing book: {book}\")\n",
    "\n",
    "    semantic_dir = os.path.join(BASE, book, \"outputs\", \"semantic_pages\")\n",
    "    assets_dir   = os.path.join(BASE, book, \"outputs\", \"assets\")\n",
    "\n",
    "    if not os.path.exists(semantic_dir):\n",
    "        print(f\"âŒ Missing semantic_pages for {book}\")\n",
    "        continue\n",
    "\n",
    "    output_path = os.path.join(BASE, book, \"outputs\", \"llava_training.jsonl\")\n",
    "    ckpt_path   = os.path.join(BASE, book, \"outputs\", \"llava_checkpoint.jsonl\")\n",
    "\n",
    "    runner.process_book(\n",
    "        semantic_dir=semantic_dir,\n",
    "        assets_dir=assets_dir,\n",
    "        output_path=output_path,\n",
    "        ckpt_path=ckpt_path,\n",
    "        max_pages=max_pages,\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… ALL BOOKS COMPLETED!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec58912-0cd1-4ffb-9276-2af37869a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "\n",
    "BASE = r\"C:/Users/Admin/Desktop/vbooks\"\n",
    "\n",
    "all_lines = []\n",
    "\n",
    "for bk in os.listdir(BASE):\n",
    "    fpath = os.path.join(BASE, bk, \"outputs\", \"llava_training.jsonl\")\n",
    "    if not os.path.exists(fpath): \n",
    "        continue\n",
    "\n",
    "    print(\"collecting -->\", fpath)\n",
    "\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if ln:\n",
    "                all_lines.append(ln)\n",
    "\n",
    "print(\"total samples:\", len(all_lines))\n",
    "\n",
    "# optional shuffle\n",
    "random.shuffle(all_lines)\n",
    "\n",
    "# final merged dataset\n",
    "merged = os.path.join(BASE, \"merged_llava_training.jsonl\")\n",
    "with open(merged, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in all_lines:\n",
    "        f.write(r + \"\\n\")\n",
    "\n",
    "print(\"merged written to:\", merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060cfe41-59fc-4539-bcb4-1461de2a13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLaVA 1.5â€“7B Heavy LoRA Fine-Tuning (Image + Text, 1 GPU)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# ======== ENV VARS (NO BNB, NO NVML) ========\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"]    = \"1\"\n",
    "os.environ[\"BITSANDBYTES_DISABLE\"]      = \"1\"\n",
    "os.environ[\"PYTORCH_NO_NVML\"]           = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]      = \"0\"\n",
    "sys.modules[\"bitsandbytes\"]             = None\n",
    "\n",
    "# ======== IMPORTS ========\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from transformers import (\n",
    "    LlavaProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ============================================================\n",
    "# 0ï¸âƒ£ Seed & Device\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"ðŸ–¥ Using device:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# 1ï¸âƒ£ Paths\n",
    "# ============================================================\n",
    "RAW_JSONL = \"C:/Users/Admin/Desktop/vbooks/merged_llava_training.jsonl\"\n",
    "IMAGE_ROOT = \"C:/Users/Admin/Desktop/vbooks\"\n",
    "DATA_DIR   = \"C:/Users/Admin/Desktop/vbooks/hf_llava_all_heavy\"\n",
    "\n",
    "OUTPUT_DIR  = \"C:/Users/Admin/Desktop/vbooks/llava_outputs_bf16_heavy/ckpts\"\n",
    "ADAPTER_DIR = \"C:/Users/Admin/Desktop/vbooks/llava_outputs_bf16_heavy/adapter\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "MAX_LEN = 1536\n",
    "\n",
    "# ============================================================\n",
    "# 2ï¸âƒ£ Processor + Tokenizer\n",
    "# ============================================================\n",
    "print(\"ðŸ” Loading processor/tokenizer...\")\n",
    "processor = LlavaProcessor.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "processor.tokenizer = tokenizer\n",
    "image_token = processor.image_token\n",
    "print(\"ðŸ”¹ image_token:\", image_token)\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ============================================================\n",
    "# 3ï¸âƒ£ Dataset loader\n",
    "# ============================================================\n",
    "\n",
    "def resolve_image_path(p):\n",
    "    if not p:\n",
    "        return None\n",
    "    if os.path.isabs(p) and os.path.exists(p):\n",
    "        return p\n",
    "    cand = os.path.join(IMAGE_ROOT, p)\n",
    "    return cand if os.path.exists(cand) else None\n",
    "\n",
    "def build_segments(conv):\n",
    "    segs = []\n",
    "    first_human = True\n",
    "    for turn in conv:\n",
    "        role = turn[\"from\"]\n",
    "        text = turn[\"value\"]\n",
    "\n",
    "        # Strip <image> if present\n",
    "        if isinstance(text, str) and text.startswith(\"<image>\"):\n",
    "            parts = text.split(\"\\n\", 1)\n",
    "            text = parts[1].strip() if len(parts) == 2 else \"\"\n",
    "\n",
    "        if role == \"human\":\n",
    "            if first_human:\n",
    "                seg = f\"USER: {image_token}\\n{text}\\n\"\n",
    "                first_human = False\n",
    "            else:\n",
    "                seg = f\"USER: {text}\\n\"\n",
    "            segs.append({\"text\": seg, \"is_human\": True})\n",
    "        else:\n",
    "            seg = f\"ASSISTANT: {text}</s>\\n\"\n",
    "            segs.append({\"text\": seg, \"is_human\": False})\n",
    "    return segs\n",
    "\n",
    "def prepare_or_load():\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        ds = load_from_disk(DATA_DIR)\n",
    "        print(\"ðŸ“¦ Loaded existing dataset:\", DATA_DIR)\n",
    "        print(\"ðŸ“Š Size:\", len(ds))\n",
    "        return ds\n",
    "\n",
    "    print(\"ðŸ“¥ Loading:\", RAW_JSONL)\n",
    "    raw = load_dataset(\"json\", data_files=RAW_JSONL, split=\"train\")\n",
    "    print(\"ðŸ“Š Raw examples:\", len(raw))\n",
    "\n",
    "    processed = []\n",
    "    for ex in raw:\n",
    "        img = resolve_image_path(ex.get(\"image\"))\n",
    "        segs = build_segments(ex[\"conversations\"])\n",
    "        full = \"\".join(s[\"text\"] for s in segs)\n",
    "        processed.append({\n",
    "            \"id\": ex.get(\"id\", \"\"),\n",
    "            \"image_path\": img,\n",
    "            \"has_real_image\": img is not None,\n",
    "            \"segments\": segs,\n",
    "            \"full_text\": full,\n",
    "        })\n",
    "\n",
    "    ds = Dataset.from_list(processed)\n",
    "    ds.save_to_disk(DATA_DIR)\n",
    "    print(\"ðŸ’¾ Saved processed dataset:\", DATA_DIR)\n",
    "    return ds\n",
    "\n",
    "dataset = prepare_or_load()\n",
    "\n",
    "# ============================================================\n",
    "# 4ï¸âƒ£ Train/Val split\n",
    "# ============================================================\n",
    "split = dataset.train_test_split(test_size=0.05, seed=SEED)\n",
    "train_ds = split[\"train\"]\n",
    "val_ds   = split[\"test\"]\n",
    "\n",
    "print(f\"ðŸ§© Train: {len(train_ds)} | Val: {len(val_ds)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5ï¸âƒ£ Collate Function (FINAL FIXED VERSION)\n",
    "# ============================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    texts, images, all_segments = [], [], []\n",
    "\n",
    "    for ex in batch:\n",
    "        texts.append(ex[\"full_text\"])\n",
    "        all_segments.append(ex[\"segments\"])\n",
    "\n",
    "        if ex[\"image_path\"]:\n",
    "            try:\n",
    "                images.append(Image.open(ex[\"image_path\"]).convert(\"RGB\"))\n",
    "            except:\n",
    "                images.append(Image.new(\"RGB\", (336,336), \"white\"))\n",
    "        else:\n",
    "            images.append(Image.new(\"RGB\", (336,336), \"white\"))\n",
    "\n",
    "    # --- Let processor build the true multimodal encoding ---\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    labels = input_ids.clone()     # start fully supervised\n",
    "\n",
    "    # --- Mask HUMAN tokens (offset-walking) ---\n",
    "    for i, segs in enumerate(all_segments):\n",
    "\n",
    "        # Assuming processor adds 1 BOS token\n",
    "        offset = 1\n",
    "        seq_len = labels.size(1)\n",
    "\n",
    "        for seg in segs:\n",
    "            seg_ids = processor.tokenizer(seg[\"text\"], add_special_tokens=False).input_ids\n",
    "            L = len(seg_ids)\n",
    "\n",
    "            if L == 0:\n",
    "                continue\n",
    "            if offset >= seq_len:\n",
    "                break\n",
    "\n",
    "            end = min(offset + L, seq_len)\n",
    "\n",
    "            if seg[\"is_human\"]:\n",
    "                labels[i, offset:end] = -100  # ignore user text\n",
    "\n",
    "            offset += L\n",
    "\n",
    "        # Also mask padding positions\n",
    "        labels[i, inputs[\"attention_mask\"][i] == 0] = -100\n",
    "\n",
    "    out = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"pixel_values\": inputs[\"pixel_values\"],\n",
    "    }\n",
    "    if \"image_sizes\" in inputs:\n",
    "        out[\"image_sizes\"] = inputs[\"image_sizes\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”Ž DEBUG: Check if masking works\n",
    "# ============================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "debug_dl = DataLoader(train_ds, batch_size=1, collate_fn=collate_fn)\n",
    "dbg = next(iter(debug_dl))\n",
    "\n",
    "print(\"\\n=== DEBUG MASKING ===\")\n",
    "print(\"labels shape:\", dbg[\"labels\"].shape)\n",
    "print(\"supervised token count:\", (dbg[\"labels\"] != -100).sum().item())\n",
    "print(\"supervised fraction:\", (dbg[\"labels\"] != -100).sum().item() / dbg[\"labels\"].numel())\n",
    "print(\"======================\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 6ï¸âƒ£ Load LLaVA and Apply LoRA\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ§  Loading LLaVA base model...\")\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Freeze vision tower\n",
    "for n,p in model.named_parameters():\n",
    "    if \"vision_tower\" in n:\n",
    "        p.requires_grad = False\n",
    "print(\"ðŸŽ¯ Vision tower frozen\")\n",
    "\n",
    "# Make multi-modal projector trainable\n",
    "mm_train = 0\n",
    "for n,p in model.named_parameters():\n",
    "    if \"multi_modal_projector\" in n:\n",
    "        p.requires_grad = True\n",
    "        mm_train += p.numel()\n",
    "print(\"ðŸ”¹ mm_projector trainable:\", mm_train)\n",
    "\n",
    "# LoRA targets for HF LLaVA\n",
    "lconf = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lconf)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"ðŸ§® Trainable Params: {trainable} / {total} ({100*trainable/total:.4f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# 7ï¸âƒ£ Training Args\n",
    "# ============================================================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=20,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=(not use_bf16),\n",
    "    bf16=use_bf16,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 8ï¸âƒ£ Trainer\n",
    "# ============================================================\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 9ï¸âƒ£ Training\n",
    "# ============================================================\n",
    "\n",
    "ckpts = glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint-*\"))\n",
    "resume_ckpt = sorted(ckpts)[-1] if ckpts else None\n",
    "\n",
    "trainer.train(resume_from_checkpoint=resume_ckpt)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”Ÿ Save Adapter\n",
    "# ============================================================\n",
    "\n",
    "trainer.save_model(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_DIR)\n",
    "print(\"âœ… LoRA adapter saved:\", ADAPTER_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 1ï¸âƒ£1ï¸âƒ£ PLOTS\n",
    "# ============================================================\n",
    "\n",
    "logs = trainer.state.log_history\n",
    "train_steps, train_losses = [], []\n",
    "eval_steps, eval_losses = [], []\n",
    "\n",
    "for r in logs:\n",
    "    if \"loss\" in r: \n",
    "        train_steps.append(r[\"step\"])\n",
    "        train_losses.append(r[\"loss\"])\n",
    "    if \"eval_loss\" in r:\n",
    "        eval_steps.append(r[\"step\"])\n",
    "        eval_losses.append(r[\"eval_loss\"])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_steps, train_losses, label=\"train\")\n",
    "plt.plot(eval_steps, eval_losses, label=\"eval\")\n",
    "plt.legend(); plt.grid(); plt.xlabel(\"step\"); plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIII Kernel",
   "language": "python",
   "name": "aiii"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
